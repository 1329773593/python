1 决策树
在解释随机森林前，需要先提一下决策树。

image.png

上面的图片可以直观的表达决策树的逻辑。选择好瓜的时候,我们首先要判断一个西瓜的纹理，如果纹理很模糊，那么这个挂一定是坏瓜；如果这个瓜的纹理稍微模糊，就去判断这个西瓜的触感怎么样。如果这个瓜的纹理比较清晰，那么接下来我们可以通过观察这个瓜的各个部分比如根蒂、色泽以及触感去一步一步判断一个瓜的好坏。这个就是决策树在分类问题中非常典型的例子。当决策树用于回归问题的时候，每个叶子节点就是一个一个实数值。

2 bagging集成
机器学习算法中有两类典型的集成思想：bagging和boosting。

bagging是一种在原始数据集上，通过有放回抽样分别选出k个新数据集，来训练分类器的集成算法。分类器之间没有依赖关系。

随机森林属于bagging集成算法。通过组合多个弱分类器，集思广益，使得整体模型具有较高的精确度和泛化性能。

3 随机森林
image.png

3.1 概述
随机森林是一种由决策树构成的集成算法，不同决策树之间没有关联。

当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。如果进行的是回归的任务，最后结果取的是平均值。

举个简单的例子，我要决定五一去重庆游玩的景点。于是我询问了一位重庆本地的朋友，她给了我一些建议。这是典型的决策树算法。我的朋友根据自己的经验，告诉我可以去哪些景点游玩。之后，我又问了很多在重庆待过的朋友，他们推荐了自己去过的景点。然后我最终选择了被推荐次数最多的景点，这就是典型的随机森林算法。

所以理论上，随机森林的表现一般要优于单一的决策树，因为随机森林的结果是通过多个决策树结果投票来决定最后的结果。并且，由于随机性，随机森林对于降低模型方差效果显著。故随机森林一般不需要额外剪枝，就能取得较好的泛化性能。

3.2 步骤
image.png

假如有N个样本，从原始样本中随机且有放回地抽取N个样本，这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
按照步骤1~3建立大量的决策树，这样就构成了随机森林了。
3.3 随机森林的随机性
数据集的随机选取：从原始的数据集中采取有放回的抽样（bagging），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。

待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。

为什么要随机抽样训练集？

如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的

为什么要有放回地抽样？

如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

树的个数是越多越好吗？

树越多代表整体的能力越强，但是，如果建立太多的树模型，会导致整体效率有所下降，还需考虑时间成本。在实际问题中，树模型的个数一般取100～200个，继续增加下去，效果也不会发生明显改变。

3.4 分类效果（错误率）与两个因素有关
森林中任意两棵树的相关性：相关性越大，错误率越大；
森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
3.5 优缺点
优点

采用了集成算法，精度优于大多数单模型算法
在测试集上表现良好，两个随机性的引入降低了过拟合风险​
树的组合可以让随机森林处理非线性数据
训练过程中能检测特征重要性，是常见的特征筛选方法​
每棵树可以同时生成，并行效率高，训练速度快
可以自动处理缺省值​
缺点

随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。
对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的
4 案例及其实现
4.1 案例介绍
在这里采用跟决策树相同的案例进行一个准确度的对比。

根据红酒的颜色强度，脯氨酸，类黄酮等变量，生成一个能够区分琴酒，雪莉，贝尔摩德三种品种的红酒的随机森林。

image.png

4.2 案例操作
image.png

4.3 结果对比
输出结果1：模型参数​

image.png

相同案例决策树的训练时间为0.004s，我们可以看到随机森林的训练时间明显多于单个决策树。

输出结果2：特征重要性

image.png 随机森林vs决策树

上柱形图或表格展示了各特征的重要性比例

输出结果4：模型评估结果

随机森林:

image.png

决策树：

image.png

​训练集的各分类评价指标都等于1，说明模型在训练集的分类完全正确，没有分类错误的样本。

而测试集的各分类评价指标随机森林均大于决策树，说明随机森林的分类效果更好。
