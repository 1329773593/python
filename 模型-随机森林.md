以下是基于随机森林原始论文（Breiman, 2001）和算法核心原理整理的完整伪代码，涵盖了训练和预测全过程：

------

### **随机森林（Random Forest）伪代码**

#### **1. 训练阶段**

```
输入:
  D = {(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}  # 训练数据集，xᵢ为样本特征，yᵢ为标签
  T: 决策树的数量
  m: 每棵树分裂时随机选择的特征数（通常 m = √M，M为总特征数）
  node_size: 叶节点最小样本数（控制树生长深度）

输出:
  随机森林模型 RF = {tree₁, tree₂, ..., treeₜ}

过程:
for t = 1 to T:
  // 1. 自助采样（Bootstrap Sampling）
  Dₜ ← 从 D 中有放回随机采样 n 个样本（允许重复样本）

  // 2. 构建决策树
  treeₜ ← grow_tree(Dₜ, m, node_size)

end for

函数 grow_tree(Dₜ, m, node_size):
  if Dₜ 中样本数 ≤ node_size 或 所有样本标签相同:
    创建叶节点，其标签 = Dₜ 中多数样本的标签
    return 叶节点

  else:
    // 随机选择 m 个特征作为候选分裂特征
    F ← 从所有 M 个特征中随机选择 m 个特征

    // 寻找最优分裂点
    (best_feat, best_val) ← None
    best_gain ← -∞
    for each feature f in F:
      计算 f 所有可能分裂点（如基尼指数或信息增益）
      选择使不纯度下降最大的分裂点 (f, val)
      if 不纯度下降 > best_gain:
        best_gain ← 不纯度下降
        best_feat ← f
        best_val ← val

    // 分裂节点
    left_data ← Dₜ 中满足 best_feat ≤ best_val 的样本
    right_data ← Dₜ 中满足 best_feat > best_val 的样本

    创建内部节点，分裂规则为 best_feat ≤ best_val
    左子树 ← grow_tree(left_data, m, node_size)
    右子树 ← grow_tree(right_data, m, node_size)

    return 内部节点（含分裂规则及左右子树）
  end if
```

#### **2. 预测阶段**

```
输入:
  RF: 训练好的随机森林模型
  x: 待预测样本

输出:
  y_pred: 样本 x 的预测结果（分类为多数投票，回归为均值）

过程:
分类任务:
  votes ← [0, 0, ..., 0]  # 初始化各类别投票计数器
  for each tree in RF:
    yᵢ ← tree 对 x 的预测类别
    votes[yᵢ] += 1

  y_pred ← argmax(votes)  # 选择票数最多的类别

回归任务:
  predictions ← []
  for each tree in RF:
    predᵢ ← tree 对 x 的预测值
    predictions.append(predᵢ)
  y_pred ← mean(predictions)  # 取所有树预测的平均值
```

------

### **关键参数说明**

1. **T（树数量）**
   - 增加 T 可降低模型方差，但计算成本增加（一般 T ≥ 100）。
2. **m（随机特征数）**
   - 控制树间多样性，通常取 `m = √M`（分类）或 `m = M/3`（回归）。
3. **node_size（叶节点最小样本数）**
   - 较小值使树更复杂，可能过拟合；较大值则欠拟合（默认为 1）。

------

### **算法核心特点**

1. **双重随机性**
   - 样本随机性（Bootstrap 采样） + 特征随机性（分裂时选子集），降低过拟合风险。
2. **完全生长树**
   - 每棵树不剪枝（除非节点样本数 ≤ `node_size`），保证高拟合度。
3. **无偏误差估计**
   - 未进入 Bootstrap 样本的数据（约 36.8%）作为袋外数据（OOB），用于估计泛化误差。

> 此伪代码参考了原始论文 *Random Forests*(Breiman, 2001) 和通用实现原理。实际库（如 scikit-learn）可能包含优化细节（如并行计算）。

