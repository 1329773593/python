以下是基于孤立森林（Isolation Forest）原始论文（Liu et al., 2008）及算法核心原理整理的完整伪代码，包含训练和预测全过程：

------

### **孤立森林（Isolation Forest）伪代码**

#### **1. 训练阶段（构建森林）**

```
输入:
  D = {x₁, x₂, ..., xₙ}          # 训练数据集，xᵢ为样本特征（无标签）
  T: 孤立树的数量（默认100）
  ψ: 子采样大小（默认256）
  H: 树高度限制（默认 ceiling(log₂(ψ))）

输出:
  孤立森林模型 IF = {iTree₁, iTree₂, ..., iTreeₜ}

过程:
for t = 1 to T:
  // 1. 子采样（Subsampling）
  Dₜ ← 从 D 中随机无放回采样 ψ 个样本

  // 2. 构建孤立树（iTree）
  iTreeₜ ← BuildTree(Dₜ, current_height=0, H)
end for

函数 BuildTree(S, current_height, H):
  if current_height ≥ H 或 |S| ≤ 1:    // 停止条件：达到高度限制或样本数≤1
    创建外部节点（叶节点）
    返回节点结构：{size ← |S|}
  else:
    // 随机选择特征和分割点
    q ← 随机选择特征索引（从所有特征中均匀抽样）
    p ← 随机在特征q的[min(Sₑ), max(Sₑ)]范围内选择分割值

    // 划分左右子树
    S_left ← {x ∈ S | x_q < p}
    S_right ← {x ∈ S | x_q ≥ p}

    // 递归构建子树
    left_child ← BuildTree(S_left, current_height+1, H)
    right_child ← BuildTree(S_right, current_height+1, H)

    创建内部节点
    返回节点结构：{split_att ← q, split_value ← p, left ← left_child, right ← right_child}
  end if
```

#### **2. 预测阶段（计算异常分数）**

```
输入:
  IF: 训练好的孤立森林模型
  x: 待预测样本
  n: 训练集样本总数（用于标准化）

输出:
  s(x, n): 异常分数（值越接近1表示越异常）

过程:
// 1. 计算路径长度（Path Length）
function PathLength(x, tree, current_height):
  if tree 是外部节点:
    // 修正：外部节点的路径长度为当前高度 + 调整项 c(tree.size)
    return current_height + c(tree.size)   // c(k) 见下方定义
  else:
    // 根据分裂规则选择子树
    if x[tree.split_att] < tree.split_value:
      return PathLength(x, tree.left, current_height+1)
    else:
      return PathLength(x, tree.right, current_height+1)
    end if
  end if

// 2. 计算平均路径长度
h_avg(x) ← 0
for each tree in IF:
  h_avg(x) ← h_avg(x) + PathLength(x, tree, 0)
end for
h_avg(x) ← h_avg(x) / T   // T为树的数量

// 3. 计算异常分数
c(n) = 2H(n-1) - 2(n-1)/n  // H(k) ≈ ln(k) + 0.5772156649（欧拉常数）
s(x, n) = 2^{-h_avg(x) / c(n)}
```

------

### **关键函数与参数说明**

| **符号**    | **含义**                                                     |
| ----------- | ------------------------------------------------------------ |
| **c(k)**    | 修正函数，用于调整外部节点的路径长度。其中： - `H(k)`为调和数，近似为 `ln(k) + 0.5772156649` - 公式：`c(k) = 2H(k-1) - 2(k-1)/k` |
| **ψ**       | 子采样大小（默认256）。论文证明：ψ=256可平衡效率与准确性，增大ψ不会显著提升性能 |
| **H**       | 树高度限制（默认 `ceiling(log₂(ψ))`）。限制树生长以减少计算量，因异常点通常早期被隔离 |
| **s(x, n)** | 异常分数： - 接近 **1** → 高度异常（路径短） - 接近 **0** → 正常样本（路径长） - 0.5左右 → 无明显异常 |

------

### **算法核心特点**

1. **双重随机性**
   - **特征随机**：每棵树随机选择分裂特征（无需计算信息增益）
   - **样本随机**：每棵树仅使用子采样样本（ψ ≪ n），降低计算复杂度
2. **异常隔离原理**
   - 异常点因特征值远离正常范围，在树中路径长度短（`h_avg(x)`小）
   - 路径长度通过 `c(n)`标准化，消除样本规模影响
3. **高效性与适用性**
   - 时间复杂度：O(T·ψ·H) ≈ **线性复杂度**（T, ψ, H 均为常数）
   - 无需距离计算或密度估计，适用于高维数据

------

### **参数调优参考**

| 参数              | 建议值/说明                                                  |
| ----------------- | ------------------------------------------------------------ |
| `n_estimators`(T) | 默认100。增加可提升稳定性，但>100后收益递减                  |
| `max_samples`(ψ)  | 默认256。若数据量大可增至512，但避免超过1024（效率下降）     |
| `contamination`   | 数据集中异常点的预估比例（默认0.1）。影响 `s(x,n)`的阈值划分 |
| `max_features`    | 默认使用全部特征。高维数据可设为 `sqrt(d)`（d为特征数）      |

> 此伪代码参考原始论文 *Isolation Forest*(Liu et al., 2008) 及通用实现（如scikit-learn）。实际应用中可结合可视化（如决策边界图）验证异常点分布 。
